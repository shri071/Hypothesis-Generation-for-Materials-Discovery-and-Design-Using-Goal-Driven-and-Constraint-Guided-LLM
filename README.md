# Hypothesis Generation for Materials Discovery and Design Using Goal-Driven and Constraint-Guided LLM Agents
![figure_1_matsci_increased_font drawio (1) drawio (1)](https://github.com/user-attachments/assets/d2c2c1c7-a5e4-4c79-8696-822a2d5c31e5)

Overview of our iterative hypothesis generation and evaluation pipeline (ACCELMAT). Starting from an input 
prompt and a knowledge graph, the Hypotheses Generator (GPT-4o) proposes 20 hypotheses, which are then
reviewed by three critics–GPT-4o, Claude-3.5-Sonnet, and Gemini-1.5-Flash. Their feedback is consolidated
by the Summarizer (GPT-4o); if unanimous agreement is not reached, the hypotheses along with critic feedback
are fed back to the Hypotheses Generator for refinement and are re-evaluated by the critics. Once approved,
the final hypotheses proceed to the Evaluation Agent (OpenAI-o1-preview) for scoring

Description of ACCELMAT Framework:

1). Hypotheses Generation Agent (HGA): Given the Goal Statement and Constraints, the HGA generates multiple hypotheses, accompanied by reasoning for each. This agent is
powered by GPT-4o.


2). Critic Agents (CA): The second component consists of three Critic Agents–GPT-4o, Claude-3.5-Sonnet Anthropic, and Gemini-1.5-Flash. These
agents are provided with the hypotheses generated by the HGA, goal statement, and constraints. Their role is to evaluate each hypothesis, assessing its alignment with the goal and
constraints. Each critic gives detailed feedback to guide subsequent hypothesis refinement cycles.

3). Summarizer Agent (SA): The Summarizer Agent consolidates and organizes the feedback from all three CAs into a structured format. It then provides this comprehensive feed-
back to the HGA to guide the refinement process. We use GPT-4o as the SA.

4). Evaluation Agent (EA): The Evaluation Agent is used to evaluate the closeness and quality of the generated hypotheses. We use OpenAI-o1-preview as our evaluation agent.
